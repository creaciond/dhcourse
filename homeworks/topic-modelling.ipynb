{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я честно не придумала, на каком корпусе можно протестировать то, что у меня получится, и решила оставить корпус фантастики. Вместо этого у меня появилась гипотеза для метрики по определению количества топиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а то забуду\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка\n",
    "\n",
    "Оставлю только существительные, прилагательные, глаголы и наречия, остальное выброшу — от местоимений не очень много смысла в топик моделлинге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности под эту задачу:\n",
    "\n",
    "* вместо списка стоп-слов из NLTK я взяла тот, что лежал в репозитории курса: там есть всякие местоимения-прилагательные типа «весь», которые сильно мешались в прошлых вариациях;\n",
    "\n",
    "* вместо всех слов оставляю только существительные, прилагательные, глаголы и наречия;\n",
    "\n",
    "* дополнительно проверяю все токены, не имеют ли они отношения к имени (у pymorphy есть для этого специальные теги), чтобы тоже не замусоривало корпус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "with open(\"../data/stop_ru.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stops = set([line.strip(\"\\n\") for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_pos = set([\"NOUN\", \"ADJF\", \"ADJS\", \"COMP\", \"VERB\", \"INFN\", \"ADVB\", \"PRED\"])\n",
    "names_tags = set([\"Name\", \"Surn\", \"Patr\"])\n",
    "\n",
    "def preprocess_text(text, stops, morph):\n",
    "    lemmas_list = []\n",
    "    for token in simple_word_tokenize(text):\n",
    "        token_analysis = morph.parse(token)[0]\n",
    "        if token_analysis.tag.POS in meaningful_pos:\n",
    "            if (token_analysis.normal_form not in stops) \\\n",
    "            and (names_tags not in token_analysis.tag):\n",
    "                lemmas_list.append(token_analysis.normal_form)\n",
    "    text_lemmatized = \" \".join(lemmas_list)\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [1:01:29<00:00, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не обработались:\n",
      "1969_Zabelin_Zapiski_hronoskopista.txt\n",
      "1989_Glazkov_Vtoroj_spisok.txt\n",
      "2008_Akhmanov_Zaklinatel_dzhinnov.txt\n",
      "1977_Snegov_Kol_co_obratnogo_vremeni.txt\n",
      "2006_Romanov_Vystrel_v_zerkalo.txt\n",
      "1965_Varshavskij_Pod_nogami_Zemlya.txt\n",
      "1967_Varshavskij_Lavka_snovidenij.txt\n",
      "1987_Suhanov_Avatara.txt\n",
      "2012_Bachilo_Ne_nuzhny.txt\n",
      "1960_Zhuravlyova_Skvoz__vremya.txt\n",
      "2008_Assiriyskie_tanki_u_vrat_Memfisa.txt\n",
      "2002_Moshkov_Pobeda_uskolzaet.txt\n",
      "1924_Goncharov_Psihomashina.txt\n",
      "2009_Bachilo_Moskovskiy_okhotnik.txt\n",
      "1967_Pavlov_Korona_solnca.txt\n",
      "2008_Akhmanov_Skify_piruyut_na_zakate.txt\n",
      "2004_Bachilo_Nakhta.txt\n",
      "1930_Palej_Planeta_Kim.txt\n",
      "1959_Zabelin_V_pogone_za_ihtiozavrami.txt\n",
      "1993_Moshkov_Vozvrashchenie_iz_otpuska.txt\n",
      "1971_Zhuravlyova_Snezhnyj_most_nad_propast_yu.txt\n",
      "2007_Bachilo_Brigadir.txt\n",
      "2002_Nikolaev_Relikt.txt\n",
      "1969_Mirer_U_menia_deviat_zhiznej.txt\n",
      "1963_Varshavskij_Molekulyarnoe_kafe.txt\n",
      "1989_Glazkov_Bezdomnye_skital_cy.txt\n",
      "1997_Shumil_K_voprosu_o_smysle_zhizni.txt\n",
      "1987_Bachilo_Pomoch_mozhno_zhivym.txt\n",
      "1963_Zhuravlyova_Chelovek,_sozdavshij_Atlantidu.txt\n",
      "1992_Moshkov_Proval_rezidentury.txt\n",
      "2004_Bachilo_Akademongorodok.txt\n",
      "1968_Snegov_Vtorzhenie_v_Persej.txt\n",
      "1999_Bachilo_Vperedi_vechnost.txt\n",
      "1924_Goncharov_Mezhplanetnyj_puteshestvennik.txt\n",
      "2009_Bachilo_Lesopark.txt\n",
      "2003_Kudryavtsev_Velikaya_Udacha.txt\n",
      "2014_Bachilo_Prozhigatel.txt\n",
      "1960_Zabelin_Poyas_zhizni.txt\n",
      "1925_Goncharov_Vek_gigantov.txt\n",
      "1971_Pavlov_Cherdak_vselennoj.txt\n",
      "2003_Nikolaev_Chernaya_voda.txt\n",
      "1966_Martinov_Spiral_Vremeni.txt\n",
      "1966_Snegov_Galakticheskaya razvedka.txt\n",
      "1968_Pavlov_Akvanavty.txt\n",
      "2002_Zhuravlyova_Letyashchie_po_vselennoj.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_corpus = \"../data/fantasy_corpus\"\n",
    "processed_texts = []\n",
    "not_parsed = []\n",
    "\n",
    "for item in tqdm([fn for fn in os.listdir(path_to_corpus) if fn.endswith(\".txt\")]):\n",
    "    with open(os.path.join(path_to_corpus, item), \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            raw_text = f.read()\n",
    "            processed_texts.append(preprocess_text(raw_text, stops, morph))\n",
    "        except UnicodeDecodeError:\n",
    "            not_parsed.append(item)\n",
    "print(\"Не обработались:\\n{}\".format(\"\\n\".join(not_parsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько у нас всего текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сколько токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7,841,061'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:,}\".format(sum([len(text.split()) for text in processed_texts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_m = tfidf_vec.fit_transform(processed_texts)\n",
    "words = tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и выбор числа топиков\n",
    "\n",
    "**Идея:** я хочу попробовать оттолкнуться от семантической близости слов в топике. Если топик выделился осознанно, то семантическая близость первых 20\\* слов будет высокой.\n",
    "\n",
    "Сейчас (до того, как я начала писать код и проверять эту гипотезу) есть одно «но» — в топик могут входить не только синонимы, но и антонимы (например, слова «дорогой» и «дешёвый» вполне могут вместе попасть в финансовый топик), семантическая близость которых равна -1; тем не менее, кажется, что всё-таки даже если и так, пара антонимов не так сильно утянет значение средней семантической близости вниз, как набор слов в стиле «стул, лошадь, 23».\n",
    "\n",
    "_\\* почему 20? Если мы говорим о «авторитарных» топиках, то кажется, что такое число может отразить и важные слова с большим весом, и менее важные с весом поменьше; в случае с «демократическими» топиками 20 — достаточное число слов, чтобы зацепить разные аспекты, которые могут быть в топике._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка топиков\n",
    "\n",
    "Я буду использовать модель с такими параметрами:\n",
    "\n",
    "* обучена на текстах НКРЯ образца 2019 года,\n",
    "\n",
    "* 270 миллионов слов, объём словаря — 189 193 слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_path = \"../data/ruscorpora_upos_cbow_300_20_2019.zip\"\n",
    "with zipfile.ZipFile(w2v_model_path, \"r\") as archive:\n",
    "    stream = archive.open(\"model.bin\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как считается семантическая близость _слов в топике_: \n",
    "\n",
    "1. составляются все возможные пары из слов, \n",
    "\n",
    "2. для каждой считается сем. близость,\n",
    "\n",
    "3. берётся среднее значение всех полученных величин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_topic(topic_words, model):\n",
    "    sem_similarities = []\n",
    "    for words_pair in combinations(topic_words, 2):\n",
    "        try:\n",
    "            word1 = \"{}_{}\".format(words_pair[0], \n",
    "                                   morph.parse(words_pair[0])[0].tag.POS)\n",
    "            word2 = \"{}_{}\".format(words_pair[1], \n",
    "                                   morph.parse(words_pair[1])[0].tag.POS)\n",
    "            sim = model.similarity(word1, word2)\n",
    "        except:\n",
    "            sim = 0\n",
    "        sem_similarities.append(sim)\n",
    "    return mean(sem_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_trained_lda(all_topic_words, model):\n",
    "    all_topic_scores = []\n",
    "    for this_topic_words in all_topic_words:\n",
    "        this_topic_score = assess_topic(this_topic_words, model)\n",
    "        all_topic_scores.append(this_topic_score)\n",
    "    return all_topic_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lda(feature_m, num_topics, words):\n",
    "    # обучаем модель\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    learning_method=\"online\",\n",
    "                                    random_state=random_seed)\n",
    "    lda = lda.fit(feature_m)\n",
    "    # забираем топ-20 слов топика\n",
    "    num_top_words = 20\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "        topics.append(topic_words)\n",
    "    return lda, topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разброс топиков сделаю на порядок — от 5 до 50.\n",
    "\n",
    "_почему 5? не знаю, просто захотелось 5…_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 26s, sys: 11min 36s, total: 1h 25min 3s\n",
      "Wall time: 24min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "experiment_scores = {}\n",
    "experiment_scores_all = {}\n",
    "experiment_topics = {}\n",
    "\n",
    "for topic_num in range(5, 51):\n",
    "    lda_trained, all_topics_words = fit_lda(tfidf_m, topic_num, words)\n",
    "    all_topics_scores = assess_trained_lda(all_topics_words, model)\n",
    "    experiment_scores_all[topic_num] = all_topics_scores\n",
    "    experiment_scores[topic_num] = mean(all_topics_scores)\n",
    "    experiment_topics[topic_num] = all_topics_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все результаты сложу в отдельный файл, чтобы тетрадка не превратилась в портянку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./topic-modelling_topics.txt\", \"w\", encoding=\"utf-8\") as file_topics:\n",
    "    for topic_num in range(5, 51):\n",
    "        file_topics.write(\"{}\\n\".format(topic_num))\n",
    "        for topic in experiment_topics[topic_num]:\n",
    "            file_topics.write(\"{}\\n\".format(\", \".join(topic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты\n",
    "\n",
    "Посмотрим на результат работы модели с лучшей метрикой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее по сем. близости количество тем: 8, результат: 0.01376\n"
     ]
    }
   ],
   "source": [
    "best_score_topic_num = max(experiment_scores, key=experiment_scores.get)\n",
    "print(\"Лучшее по сем. близости количество тем: {}, результат: {:.5f}\".format(best_score_topic_num,\n",
    "                                                                            experiment_scores[best_score_topic_num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат, если честно, не очень впечатляет…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_trained, all_topics_words = fit_lda(tfidf_m, best_score_topic_num, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic no:  0\twords: боячек, ариэль, майк, несправедливость, олег, валентайна, акраб, 25ь3ый, макбет, мускулус, коллекторский, загорланить, геномодель, максик, вузина, проворонить, хорог, шлюз, флигранно, ксенофонт\n",
      "topic no:  1\twords: андрей, горелов, скотенков, форам, то, вечеровский, оказаться, валентин, учреждаться, открытый, малян, лицо, талреп, гуща, крутнувшийся, макс, просчитать, мудь, тридцатисемилетний, большой\n",
      "topic no:  2\twords: никки, шурка, то, мюргита, рука, стив, румат, земля, глаз, максим, джон, ладушкин, кирилл, апостол, психолог, арсен, ружейный, знать, солнце, большой\n",
      "topic no:  3\twords: павлыш, клэйтон, марат, игорь, перхуш, странник, бартон, доктор, барнаби, рука, брошюрный, ассасин, сериза, ротан, арбалетный, параллельно, противостояние, то, крыловой, позадавать\n",
      "topic no:  4\twords: волгин, стать, то, рэсся, самый, дело, знать, эверс, амит, родис, рифт, большой, крэл, стис, прыгнуть, вести, рука, друг, должный, высвобождение\n",
      "topic no:  5\twords: кора, фролов, глаз, стасик, арей, эдгар, игнат, престо, рука, белогурочка, стать, мэнни, керн, леся, андрей, ирка, азорес, дело, петька, соболезнование\n",
      "topic no:  6\twords: то, рука, стать, знать, самый, большой, глаз, спросить, говорить, дело, какой, голова, ирка, видеть, лицо, идти, друг, земля, арей, место\n",
      "topic no:  7\twords: хоттабыча, юрковский, вадим, волька, герд, рука, алиса, дауг, энергосборник, таис, валк, видеофонный, рем, виктор, никновенный, ива, гум, самый, виккерс, алешка\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(all_topics_words):\n",
    "    print(\"topic no: {:2}\\twords: {}\".format(i, \", \".join(topic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень похоже на внятные результаты… а если попробовать минимум?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во тем, где сем. близость минимальна: 32, результат: 0.00469\n",
      "topic no:  0\twords: ванюшка, конобей, таям, боячек, нааля, малян, гузик, несправедливость, валентайна, акраб, 25ь3ый, макбет, бартон, мускулус, коллекторский, гесера, загорланить, геномодель, максик, вузина\n",
      "topic no:  1\twords: шурка, андрей, учреждаться, белопольский, олег, оказаться, крутнувшийся, гуща, открытый, просчитать, мудь, тридцатисемилетний, шед, шахерзад, детдомовский, закаркать, рымшёныш, кора, рекомендация, папандопулос\n",
      "topic no:  2\twords: олег, солнце, земля, психолог, ружейный, оперчасть, комингс, суккулента, андрей, доменико, хээхо, тратаниана, бульшуя, ирка, туманный, высоконогий, брайль, тарогойя, перс, буфетчица\n",
      "topic no:  3\twords: странник, брошюрный, лось, арбалетный, параллельно, противостояние, крыловой, позадавать, партийка, подмес, ответный, переменность, полэкран, ихтиандр, шуля, архимед, уоттер, переплавлятьный, оптоэлектронный, оставление\n",
      "topic no:  4\twords: то, рука, стать, знать, самый, большой, глаз, спросить, говорить, дело, какой, голова, видеть, лицо, ирка, идти, друг, земля, арей, место\n",
      "topic no:  5\twords: петька, арей, соболезнование, покрытыйживойтатуировка, твиттереть, облёт, томка, ботс, миражный, дилер, чагин, краюшный, чередоваться, женька, глаз, встряхнуть, мифочко, углиться, темпограда, тросовый\n",
      "topic no:  6\twords: люся, сергей, павлыш, спонсор, саныч, маэстро, осэпсон, светлана, андраковский, прохныкать, утрешнему, дедешник, взимание, ангелика, перепархивать, отверженец, радиолюбительский, спросить, печеньице, шлюзовый\n",
      "topic no:  7\twords: валерий, нонна, ирка, рука, лекс, ротан, арей, энергосборник, керн, шамнилсин, видеофонный, стив, никновенный, считаешь, знать, мефодий, обняться, подпал, место, андерс\n",
      "topic no:  8\twords: иго, юр, попридержать, управляемый, защекотывать, фрол, жизнеописание, великанчик, чмокать, колоратура, хлопчатобумажный, девчушка, большеничий, аллариха, интоксикация, эллингтон, столысый, 211нр, тетива, шмонательный\n",
      "topic no:  9\twords: людочка, друкмашин, клык, цех, икар, бриг, гипереть, куприянов, пробабилитность, деятельный, мамон, вальс, конфузливость, убеждаться, загораживаться, стрэтфорд, опуп, благоволить, сеул, спуститься\n",
      "topic no: 10\twords: обзаведение, стасик, тика, русскоговорящий, журавлев, полунива, шелание, ветошный, карадыгнуть, взвешенность, четырехсотмиллионный, перекокошить, сыскной, красота, латинский, спарка, правительница, обережный, кухмистерский, прибежище\n",
      "topic no: 11\twords: герд, гага, самосудно, престо, госрезиденция, мариам, лий, конголезский, айтматов, шажок, озоровать, вышняго, исчертить, бюро, любвеобильный, _всё, фон, увлекать, присядка, сверка\n",
      "topic no: 12\twords: юрковский, дауг, быков, поединок, антошин, дверъ, злосчастный, рудболка, моллара, мартихор, хорёк, жилин, продленка, сиротство, спотыкание, перевести, отшельнический, катюшкин, топливный, дописываться\n",
      "topic no: 13\twords: штирнер, максим, эльза, готлиб, мюргита, таня, рука, штирнера, стать, то, знать, валентин, сугубова, наш, таис, сверхблагоустроенный, мимоходом, мява, большой, синтетик\n",
      "topic no: 14\twords: ихтиандра, марконь, зурита, антошка, пек, сальватор, кап, бальтазар, сеня, гуттиэре, отбушевать, копытыча, кристо, перхуш, олик, крепостица, тачка, николсон, барракуда, довоенному\n",
      "topic no: 15\twords: ирка, арей, мефодий, улита, эссиорх, дафный, мефа, буслаев, валькирия, ромасюсик, эйдос, прасковья, августин, лигула, корнелий, ната, мошкина, чимодан, мрак, рука\n",
      "topic no: 16\twords: митрофан, румат, резкина, асгарда, рэсся, сериза, усачев, гхор, ким, вершителемсудьба, лунатик, урата, пропаровозить, маловодье, придыхая, город, клиент, эгинуть, вражий, веда\n",
      "topic no: 17\twords: ива, ротан, апостол, марат, выдрать, ширяев, плесси, вышеназванный, херовин, ляскнуть, крэл, вицекарреон, беспонтовыя, бревенчатый, дафна, насреддин, бомборакета, грек, корь, аурный\n",
      "topic no: 18\twords: филипп, борменталь, застегнуться, лидочка, молдром, 3ин, магнитометр, ремень, василий, божество, сафар, клаксон, валерка, инкубатор, дюзовома, вырез, самый, опьянеть, пёс, георгий\n",
      "topic no: 19\twords: амит, идол, проводник, сарафанчик, напутствие, потихоньку, волнорез, клэйтон, лукинишний, передачка, десерт, кат, баснецов, вышинский, перегрузочный, тжжж, заокиянский, декольте, порось, убаюкиваться\n",
      "topic no: 20\twords: корнелий, кондратьев, морозов, арей, леся, ирка, улита, эссиорх, рука, акса, гуам, горгий, кора, самый, атлантида, большой, мобь, стать, знать, то\n",
      "topic no: 21\twords: гречь, увар, дримма, эдерс, волька, кондрат, мартин, сирано, алешка, дыркорыло, пресмыканец, авиаучилище, фокс, заклеиться, фрэнк, рука, дверца, аренда, переход, антисемит\n",
      "topic no: 22\twords: толика, мишка, дизов, автокатафалк, герц, наметаться, берил, замковый, таить, приляпать, туловить, шестиэтажный, воодушевление, пещера, методически, главый, обламыватель, прилично, щёточка, благосклонно\n",
      "topic no: 23\twords: месть, обнажённость, боп, ниоб, клепыча, названый, психопатический, эндшпиль, передничек, заскакивать, радиозапрос, перещелкивать, меддиагностик, оный, антисоветчина, колесников, консамент, мессершмитт, шиза, непафосный\n",
      "topic no: 24\twords: серёжка, герд, откланиваться, мошка, вестный, переносчик, остафалиться, одушевление, необжитый, вселунный, пчелиный, сарделька, барельефный, рант, холидэев, фриковыя, овчинка, ниоб, блажь, стадионай\n",
      "topic no: 25\twords: сашка, варенуха, то, волгин, немощь, подсматривать, ратцингер, аспада, зайце, жилконтора, гиерон, радиоузел, горячий, белков, токарев, клиника, качение, поглотитель, пернач, лиофильный\n",
      "topic no: 26\twords: лубянкина, павлыш, бабекус, андрей, барбарис, ассасин, писновать, торндайк, байконур, рэж, вечёрка, избежать, афганский, восьминогий, рыбец, палкин, макс, двухколейный, октина, перепоручить\n",
      "topic no: 27\twords: проучиться, казнь, панхард, шебуршить, дистан, турбюро, цандёр, завздыхать, выбивалка, качели, таиться, никола, квадрать, разгуляться, бэйль, недоучиться, бронь, ледяный, неполнорукий, второразрядный\n",
      "topic no: 28\twords: юля, фаддейк, арсен, ладушкин, широков, колодник, слопный, синяев, фа, дейк, злонамеренность, невсамделишный, всеохватывающий, папироска, вражий, повеса, астрофизический, афины, политехникум, дный\n",
      "topic no: 29\twords: черномордый, златокудрый, седан, чахоточно, родис, суртугой, царевна, доупиваться, большой, олтров, робятам, дроида, окнаукрасить, половецкий, отдариться, антигуманность, самоуважение, никки, чадный, непредвидимый\n",
      "topic no: 30\twords: макс, кондратьев, вельта, то, мобь, ариэль, тыца, профессор, агата, амаль, стать, кленовый, ирина, рука, самый, ганс, кирилл, голова, артём, видеть\n",
      "topic no: 31\twords: гесера, аннели, волгин, вампир, то, завулоно, дозор, рокамора, говорить, шрейер, дауг, лера, рука, знать, быков, эдинбург, друг, виктор, какой, светлана\n"
     ]
    }
   ],
   "source": [
    "worst_score_topic_num = min(experiment_scores, key=experiment_scores.get)\n",
    "print(\"Кол-во тем, где сем. близость минимальна: {}, результат: {:.5f}\".format(worst_score_topic_num,\n",
    "                                                                            experiment_scores[worst_score_topic_num]))\n",
    "lda_trained, all_topics_words = fit_lda(tfidf_m, worst_score_topic_num, words)\n",
    "for i, topic in enumerate(all_topics_words):\n",
    "    print(\"topic no: {:2}\\twords: {}\".format(i, \", \".join(topic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё равно не то чтобы были осмысленные топики. Наверное, это связано с тем, что в фантастике много всяких несуществующих слов — да и имён осталось достаточно… наверное, такие несуществующие слова тоже нужно было убрать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Бонус:_ ничто не ново под луной — семантику прикручивали ещё в 2013, но чуть иначе — [сначала считали усреднённый вектор-центроид топика, потом считали косинусную близость каждого слова в топике к этому центроиду](https://www.researchgate.net/publication/235974307_Evaluating_Topic_Coherence_Using_Distributional_Semantics)), но результат был сравнительно приятный — цифрмы от более принятой метрики PMI там приятные. Единственное «но» — данные там были не литературные, а газетные (20 Newsgroups и прочая классика жанра для классификации) — там тоже будет проблема с именами, но нет проблемы с несуществующей лексикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что ещё можно было бы попробовать сделать?\n",
    "\n",
    "Пока я писала плюсы и минусы своей идеи, мне пришло в голову, что тут хорошо бы справилась какая-нибудь онтология или ворднет — благодаря тому, что в них реализована идея уровневости; можно было бы смотреть, совпадают ли у слов в топиках гиперонимы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
